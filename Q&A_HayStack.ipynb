{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Q&A-HayStack.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7NjHIIYgsNI"
      },
      "source": [
        "!pip install flask\n",
        "!pip install flask_ngrok\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAFuLLfiZc_w"
      },
      "source": [
        "!apt-get install xpdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwnD183dTN_h"
      },
      "source": [
        "mkdir -p data/amazon"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NabH-dPPLb5e"
      },
      "source": [
        "!wget -P ./data/amazon/ https://s2.q4cdn.com/299287126/files/doc_financials/2020/q4/Amazon-Q4-2020-Earnings-Release.pdf\n",
        "!wget -P ./data/amazon/ https://s2.q4cdn.com/299287126/files/doc_financials/2020/q3/AMZN-Q3-2020-Earnings-Release.pdf\n",
        "!wget -P ./data/amazon/ https://s2.q4cdn.com/299287126/files/doc_financials/2020/q2/Q2-2020-Amazon-Earnings-Release.pdf\n",
        "!wget -P ./data/amazon/ https://s2.q4cdn.com/299287126/files/doc_financials/2020/Q1/AMZN-Q1-2020-Earnings-Release.pdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOA6CJuyTcZ9",
        "outputId": "88abd5e8-317c-422b-90ee-50072a50cc97"
      },
      "source": [
        "ls data/amazon"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Amazon-Q4-2020-Earnings-Release.pdf  AMZN-Q3-2020-Earnings-Release.pdf\n",
            "AMZN-Q1-2020-Earnings-Release.pdf    Q2-2020-Amazon-Earnings-Release.pdf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIryrBafTh0r"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5lbmcjSTqMx"
      },
      "source": [
        "# Install the latest master of Haystack\n",
        "!pip install git+https://github.com/deepset-ai/haystack.git\n",
        "!pip install urllib3==1.25.4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPuklRtWUF_q"
      },
      "source": [
        "from haystack.preprocessor.cleaning import clean_wiki_text\n",
        "from haystack.preprocessor.utils import convert_files_to_dicts, fetch_archive_from_http\n",
        "from haystack.reader.farm import FARMReader\n",
        "from haystack.reader.transformers import TransformersReader\n",
        "from haystack.utils import print_answers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6TagJrn5-x1"
      },
      "source": [
        "# In Colab / No Docker environments: Start Elasticsearch from source\n",
        "! wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.2-linux-x86_64.tar.gz -q\n",
        "! tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz\n",
        "! chown -R daemon:daemon elasticsearch-7.9.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qk_zq3GglF4O"
      },
      "source": [
        "import os\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "es_server = Popen(['elasticsearch-7.9.2/bin/elasticsearch'],\n",
        "                   stdout=PIPE, stderr=STDOUT,\n",
        "                   preexec_fn=lambda: os.setuid(1)  # as daemon\n",
        "                  )\n",
        "# wait until ES has started\n",
        "! sleep 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfo_pcZeVVcX"
      },
      "source": [
        "# Connect to Elasticsearch\n",
        "\n",
        "from haystack.document_store.elasticsearch import ElasticsearchDocumentStore\n",
        "document_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=\"document\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J3Qb9HRVgDT"
      },
      "source": [
        "directory = \"data/amazon/\"\n",
        "from haystack.file_converter.pdf import PDFToTextConverter\n",
        "from haystack.preprocessor.preprocessor import PreProcessor\n",
        "\n",
        "converter = PDFToTextConverter(remove_numeric_tables=True, valid_languages=[\"de\",\"en\"])\n",
        "\n",
        "processor = PreProcessor(clean_empty_lines=True,\n",
        "                         clean_whitespace=True,\n",
        "                         clean_header_footer=True,\n",
        "                         split_by=\"word\",\n",
        "                         split_length=200,\n",
        "                         split_respect_sentence_boundary=True)\n",
        "docs = []\n",
        "for filename in os.listdir(directory):\n",
        "\n",
        "    # Run the conversion on each file (PDF -> 1x doc)\n",
        "    d = converter.convert(os.path.join(directory, filename), meta=None)\n",
        "\n",
        "    # clean and split each dict (1x doc -> multiple docs)\n",
        "    d = processor.process(d)\n",
        "    docs.extend(d)\n",
        "\n",
        "# Let's have a look at the first 3 entries:\n",
        "print(docs[:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKJ6IF-SZJ53"
      },
      "source": [
        "document_store.write_documents(docs)\n",
        "\n",
        "# Now, let's write the dicts containing documents to our DB.\n",
        "document_store.write_documents(docs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5lzILNCaLEP"
      },
      "source": [
        "from haystack.retriever.sparse import ElasticsearchRetriever\n",
        "retriever = ElasticsearchRetriever(document_store=document_store)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MK5GY2QaKzA"
      },
      "source": [
        "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPyxxZ4JaTJR"
      },
      "source": [
        "from haystack.pipeline import ExtractiveQAPipeline\n",
        "pipe = ExtractiveQAPipeline(reader, retriever)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOdWDQ2DaSyy"
      },
      "source": [
        "prediction = pipe.run(query=\"Who is the new CEO?\", top_k_retriever=20, top_k_reader=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwfPmmt6aieK"
      },
      "source": [
        "print_answers(prediction, details=\"minimal\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y9ZhCluiuoH"
      },
      "source": [
        "import json\n",
        "import os\n",
        "import logging\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from flask_cors import CORS\n",
        "from flask import Flask, request, jsonify\n",
        "from haystack import Finder\n",
        "from haystack.preprocessor.cleaning import clean_wiki_text\n",
        "from haystack.preprocessor.utils import convert_files_to_dicts\n",
        "from haystack.reader.farm import FARMReader\n",
        "from haystack.document_store.elasticsearch import ElasticsearchDocumentStore\n",
        "from haystack.file_converter.pdf import PDFToTextConverter\n",
        "from haystack.retriever.dense import DensePassageRetriever\n",
        "from haystack.retriever.sparse import ElasticsearchRetriever\n",
        "\n",
        "#application settings\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_so1R9VUqE_I"
      },
      "source": [
        "@app.route('/search',methods=['GET', 'POST'])\n",
        "def search():\n",
        "    \"\"\"Return the n answers.\"\"\"\n",
        "\n",
        "    question = request.get_json()\n",
        "    question = question['query']\n",
        " \n",
        "    #initialization of the Haystack Elasticsearch document storage\n",
        "    document_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=\"document\")\n",
        "\n",
        "    # using pretrain model\n",
        "    #reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)\n",
        "\n",
        "    # Finder sticks together reader and retriever\n",
        "    # in a pipeline to answer our actual questions.\n",
        "    finder = Finder(reader, retriever)\n",
        "\n",
        "    prediction = finder.get_answers(question=question, top_k_retriever=10, top_k_reader=5)\n",
        "    answer = []\n",
        "    for res in prediction['answers']:\n",
        "        answer.append(res['answer'])\n",
        "\n",
        "    return json.dumps({'status':'success','message': 'Process succesfully', 'result': answer})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CprwMspBrRDT"
      },
      "source": [
        "run_with_ngrok(app)\n",
        "app.run()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
